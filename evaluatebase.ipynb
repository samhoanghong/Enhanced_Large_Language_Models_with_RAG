{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles: ['Multilingual Contextual Affective Analysis of LGBT People Portrayals in\\n  Wikipedia', 'Adapting Coreference Resolution for Processing Violent Death Narratives', 'Locating Information Gaps and Narrative Inconsistencies Across\\n  Languages: A Case Study of LGBT People Portrayals on Wikipedia', 'Eliciting Information from Sensitive Survey Questions', 'Analyzing Right-wing YouTube Channels: Hate, Violence and Discrimination', 'The FRENK Datasets of Socially Unacceptable Discourse in Slovene and\\n  English', \"Can't say cant? Measuring and Reasoning of Dark Jargons in Large\\n  Language Models\", 'OneLove beyond the field -- A few-shot pipeline for topic and sentiment\\n  analysis during the FIFA World Cup in Qatar', 'The Sensitivity of Respondent-driven Sampling Method', 'Enriching gender in PER: A binary past and a complex future']\n",
      "Abstracts: [\"  Specific lexical choices in narrative text reflect both the writer's\\nattitudes towards people in the narrative and influence the audience's\\nreactions. Prior work has examined descriptions of people in English using\\ncontextual affective analysis, a natural language processing (NLP) technique\\nthat seeks to analyze how people are portrayed along dimensions of power,\\nagency, and sentiment. Our work presents an extension of this methodology to\\nmultilingual settings, which is enabled by a new corpus that we collect and a\\nnew multilingual model. We additionally show how word connotations differ\\nacross languages and cultures, highlighting the difficulty of generalizing\\nexisting English datasets and methods. We then demonstrate the usefulness of\\nour method by analyzing Wikipedia biography pages of members of the LGBT\\ncommunity across three languages: English, Russian, and Spanish. Our results\\nshow systematic differences in how the LGBT community is portrayed across\\nlanguages, surfacing cultural differences in narratives and signs of social\\nbiases. Practically, this model can be used to identify Wikipedia articles for\\nfurther manual analysis -- articles that might contain content gaps or an\\nimbalanced representation of particular social groups.\\n\", \"  Coreference resolution is an important component in analyzing narrative text\\nfrom administrative data (e.g., clinical or police sources). However, existing\\ncoreference models trained on general language corpora suffer from poor\\ntransferability due to domain gaps, especially when they are applied to\\ngender-inclusive data with lesbian, gay, bisexual, and transgender (LGBT)\\nindividuals. In this paper, we analyzed the challenges of coreference\\nresolution in an exemplary form of administrative text written in English:\\nviolent death narratives from the USA's Centers for Disease Control's (CDC)\\nNational Violent Death Reporting System. We developed a set of data\\naugmentation rules to improve model performance using a probabilistic data\\nprogramming framework. Experiments on narratives from an administrative\\ndatabase, as well as existing gender-inclusive coreference datasets,\\ndemonstrate the effectiveness of data augmentation in training coreference\\nmodels that can better handle text data about LGBT individuals.\\n\", \"  To explain social phenomena and identify systematic biases, much research in\\ncomputational social science focuses on comparative text analyses. These\\nstudies often rely on coarse corpus-level statistics or local word-level\\nanalyses, mainly in English. We introduce the InfoGap method -- an efficient\\nand reliable approach to locating information gaps and inconsistencies in\\narticles at the fact level, across languages. We evaluate InfoGap by analyzing\\nLGBT people's portrayals, across 2.7K biography pages on English, Russian, and\\nFrench Wikipedias. We find large discrepancies in factual coverage across the\\nlanguages. Moreover, our analysis reveals that biographical facts carrying\\nnegative connotations are more likely to be highlighted in Russian Wikipedia.\\nCrucially, InfoGap both facilitates large scale analyses, and pinpoints local\\ndocument- and fact-level information gaps, laying a new foundation for targeted\\nand nuanced comparative language analysis at scale.\\n\", '  This paper considers how to elicit information from sensitive survey\\nquestions. First we thoroughly evaluate list experiments (LE), a leading method\\nin the experimental literature on sensitive questions. Our empirical results\\ndemonstrate that the assumptions required to identify sensitive information in\\nLE are violated for the majority of surveys. Next we propose a novel survey\\nmethod, called Multiple Response Technique (MRT), for eliciting information\\nfrom sensitive questions. We require all of the respondents to answer three\\nquestions related to the sensitive information. This technique recovers\\nsensitive information at a disaggregated level while still allowing arbitrary\\nmisreporting in survey responses. An application of the MRT provides novel\\nempirical evidence on sexual orientation and Lesbian, Gay, Bisexual, and\\nTransgender (LGBT)-related sentiment.\\n', '  As of 2018, YouTube, the major online video sharing website, hosts multiple\\nchannels promoting right-wing content. In this paper, we observe issues related\\nto hate, violence and discriminatory bias in a dataset containing more than\\n7,000 videos and 17 million comments. We investigate similarities and\\ndifferences between users\\' comments and video content in a selection of\\nright-wing channels and compare it to a baseline set using a three-layered\\napproach, in which we analyze (a) lexicon, (b) topics and (c) implicit biases\\npresent in the texts. Among other results, our analyses show that right-wing\\nchannels tend to (a) contain a higher degree of words from \"negative\" semantic\\nfields, (b) raise more topics related to war and terrorism, and (c) demonstrate\\nmore discriminatory bias against Muslims (in videos) and towards LGBT people\\n(in comments). Our findings shed light not only into the collective conduct of\\nthe YouTube community promoting and consuming right-wing content, but also into\\nthe general behavior of YouTube users.\\n', '  In this paper we present datasets of Facebook comment threads to mainstream\\nmedia posts in Slovene and English developed inside the Slovene national\\nproject FRENK which cover two topics, migrants and LGBT, and are manually\\nannotated for different types of socially unacceptable discourse (SUD). The\\nmain advantages of these datasets compared to the existing ones are identical\\nsampling procedures, producing comparable data across languages and an\\nannotation schema that takes into account six types of SUD and five targets at\\nwhich SUD is directed. We describe the sampling and annotation procedures, and\\nanalyze the annotation distributions and inter-annotator agreements. We\\nconsider this dataset to be an important milestone in understanding and\\ncombating SUD for both languages.\\n', \"  Ensuring the resilience of Large Language Models (LLMs) against malicious\\nexploitation is paramount, with recent focus on mitigating offensive responses.\\nYet, the understanding of cant or dark jargon remains unexplored. This paper\\nintroduces a domain-specific Cant dataset and CantCounter evaluation framework,\\nemploying Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis stages.\\nExperiments reveal LLMs, including ChatGPT, are susceptible to cant bypassing\\nfilters, with varying recognition accuracy influenced by question types,\\nsetups, and prompt clues. Updated models exhibit higher acceptance rates for\\ncant queries. Moreover, LLM reactions differ across domains, e.g., reluctance\\nto engage in racism versus LGBT topics. These findings underscore LLMs'\\nunderstanding of cant and reflect training data characteristics and vendor\\napproaches to sensitive topics. Additionally, we assess LLMs' ability to\\ndemonstrate reasoning capabilities. Access to our datasets and code is\\navailable at https://github.com/cistineup/CantCounter.\\n\", \"  The FIFA World Cup in Qatar was discussed extensively in the news and on\\nsocial media. Due to news reports with allegations of human rights violations,\\nthere were calls to boycott it. Wearing a OneLove armband was part of a planned\\nprotest activity. Controversy around the armband arose when FIFA threatened to\\nsanction captains who wear it. To understand what topics Twitter users Tweeted\\nabout and what the opinion of German Twitter users was towards the OneLove\\narmband, we performed an analysis of German Tweets published during the World\\nCup using in-context learning with LLMs. We validated the labels on human\\nannotations. We found that Twitter users initially discussed the armband's\\nimpact, LGBT rights, and politics; after the ban, the conversation shifted\\ntowards politics in sports in general, accompanied by a subtle shift in\\nsentiment towards neutrality. Our evaluation serves as a framework for future\\nresearch to explore the impact of sports activism and evolving public\\nsentiment. This is especially useful in settings where labeling datasets for\\nspecific opinions is unfeasible, such as when events are unfolding.\\n\", \"  Researchers in many scientific fields make inferences from individuals to\\nlarger groups. For many groups however, there is no list of members from which\\nto take a random sample. Respondent-driven sampling (RDS) is a relatively new\\nsampling methodology that circumvents this difficulty by using the social\\nnetworks of the groups under study. The RDS method has been shown to provide\\nunbiased estimates of population proportions given certain conditions. The\\nmethod is now widely used in the study of HIV-related high-risk populations\\nglobally. In this paper, we test the RDS methodology by simulating RDS studies\\non the social networks of a large LGBT web community. The robustness of the RDS\\nmethod is tested by violating, one by one, the conditions under which the\\nmethod provides unbiased estimates. Results reveal that the risk of bias is\\nlarge if networks are directed, or respondents choose to invite persons based\\non characteristics that are correlated with the study outcomes. If these two\\nproblems are absent, the RDS method shows strong resistance to low response\\nrates and certain errors in the participants' reporting of their network sizes.\\nOther issues that might affect the RDS estimates, such as the method for\\nchoosing initial participants, the maximum number of recruitments per\\nparticipant, sampling with or without replacement and variations in network\\nstructures, are also simulated and discussed.\\n\", '  In this article, we draw on previous reports from physics, science education,\\nand women\\'s studies to propose a more nuanced treatment of gender in physics\\neducation research (PER). A growing body of PER examines gender differences in\\nparticipation, performance, and attitudes toward physics. We have three\\ncritiques of this work: (1) it does not question whether the achievements of\\nmen are the most appropriate standard, (2) individual experiences and student\\nidentities are undervalued, and (3) the binary model of gender is not\\nquestioned. Driven by these critiques, we propose a conception of gender that\\nis more up-to-date with other fields and discuss gender-as-performance as an\\nextended example. We also discuss work on the intersection of identities [e.g.,\\ngender with race and ethnicity, socioeconomic status, lesbian, gay, bisexual,\\nand transgender (LGBT) status], much of which has been conducted outside of\\nphysics. Within PER, some studies examine the intersection of gender and race,\\nand identify the lack of a single identity as a key challenge of \"belonging\" in\\nphysics. Acknowledging this complexity enables us to further critique what we\\nterm a binary gender deficit model. This framework, which is implicit in much\\nof the gender-based PER, casts gender as a fixed binary trait and suggests that\\nwomen are deficient in characteristics necessary to succeed. Alternative models\\nof gender allow a greater range and fluidity of gender identities, and\\nhighlight deficiencies in data that exclude women\\'s experiences. We suggest new\\ninvestigations that diverge from this expanded gender framework in PER.\\n']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def search_arxiv(query, max_results=10):\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"start\": 0,\n",
    "        \"max_results\": max_results,\n",
    "        \"sortBy\": \"relevance\",\n",
    "        \"sortOrder\": \"descending\",\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def parse_arxiv_response(response):\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    \n",
    "    root = ET.fromstring(response)\n",
    "    entries = root.findall(\"{http://www.w3.org/2005/Atom}entry\")\n",
    "    \n",
    "    for entry in entries:\n",
    "        title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "        abstract = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "        titles.append(title)\n",
    "        abstracts.append(abstract)\n",
    "    \n",
    "    return titles, abstracts\n",
    "\n",
    "# Example usage\n",
    "query = \"LGBT\"\n",
    "response = search_arxiv(query)\n",
    "if response:\n",
    "    titles, abstracts = parse_arxiv_response(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_titles = [\n",
    "    \"LGBT+ Inclusivity in Physics and Astronomy: A Best Practices Guide\",\n",
    "\"Multilingual Contextual Affective Analysis of LGBT People Portrayals in   Wikipedia\",\n",
    "\"LGBTQ-AI? Exploring Expressions of Gender and Sexual Orientation in   Chatbots\",\n",
    "\"The Ethical Implications of Digital Contact Tracing for LGBTQIA+   Communities\",\n",
    "\"LGBTQ Privacy Concerns on Social Media\",\n",
    "\"Discrete Event Simulation to Evaluate Shelter Capacity Expansion Options   for LGBTQ+ Homeless Youth\",\n",
    "\"Minority Stress Experienced by LGBTQ Online Communities during the   COVID-19 Pandemic\",\n",
    "\"Detecting Harmful Online Conversational Content towards LGBTQIA+   Individuals\",\n",
    "\"Benefits and Limitations of Remote Work to LGBTQIA+ Software   Professionals\",\n",
    "\"LGBTQIA+ (In)Visibility in Computer Science and Software Engineering   Education\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores: {'rouge1': 0.28204803154648356, 'rouge2': 0.13066666666666665, 'rougeL': 0.2598229233213753}\n",
      "Detailed Scores for Each Prediction: {'rouge1': [1.0, 0.11111111111111112, 0.37037037037037035, 0, 0.10526315789473685, 0.2727272727272727, 0.23999999999999996, 0.2, 0.23529411764705882, 0.28571428571428564], 'rouge2': [1.0, 0, 0.24000000000000005, 0, 0, 0, 0, 0.06666666666666667, 0, 0], 'rougeL': [1.0, 0.11111111111111112, 0.37037037037037035, 0, 0.10526315789473685, 0.1904761904761905, 0.16666666666666666, 0.13333333333333333, 0.23529411764705882, 0.28571428571428564]}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "\n",
    "    for pred in predictions:\n",
    "        max_scores = {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "        for ref in references:\n",
    "            scores = scorer.score(ref, pred)\n",
    "            max_scores[\"rouge1\"] = max(max_scores[\"rouge1\"], scores[\"rouge1\"].fmeasure)\n",
    "            max_scores[\"rouge2\"] = max(max_scores[\"rouge2\"], scores[\"rouge2\"].fmeasure)\n",
    "            max_scores[\"rougeL\"] = max(max_scores[\"rougeL\"], scores[\"rougeL\"].fmeasure)\n",
    "        \n",
    "        results[\"rouge1\"].append(max_scores[\"rouge1\"])\n",
    "        results[\"rouge2\"].append(max_scores[\"rouge2\"])\n",
    "        results[\"rougeL\"].append(max_scores[\"rougeL\"])\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_scores = {key: sum(vals) / len(vals) for key, vals in results.items()}\n",
    "    return avg_scores, results\n",
    "\n",
    "# Example usage\n",
    "# predictions = [\"A novel approach to deep learning\", \"Deep learning for AI applications\"]\n",
    "# references = [\"A new method for deep learning\", \"Applications of AI in deep learning\"]\n",
    "\n",
    "avg_scores, detailed_scores = calculate_rouge(titles, reference_titles)\n",
    "\n",
    "print(\"Average ROUGE Scores:\", avg_scores)\n",
    "print(\"Detailed Scores for Each Prediction:\", detailed_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
